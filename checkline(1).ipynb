{"cells":[{"cell_type":"code","source":["%fs ls '/FileStore/tables/'"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df = spark.read.text('/FileStore/tables/Beckman_Sample.txt').rdd\ndf.take(2)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from operator import add\n#df2 = df.filter(lambda x: \"sample\" in x).count()\ndf3 = df.map(lambda row:row.value.split('\\t'))\n#df2 = df3.filter(lambda x: 'Sample001_MM1' in x).reduceByKey(add)\n#df2 = df.flatMap(lambda doc: [(x, 1) for x in doc.split(' ')]).reduceByKey(add)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["df3.take(2)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql import Row\n\nrdd3 = df3.map(lambda d: Row(userId= str(d[0]), \n                       typeId=str(d[1]), \n                       typeId1=str(d[2]), \n                       amount=float(d[3]), \n                       account=int(d[4]),\n                       typeaccount=int(d[5])))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df1 = spark.createDataFrame(rdd3)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.window import *\nfrom pyspark.sql.functions import row_number\ndf4 = df1.withColumn(\"row_num\", row_number().over(Window.partitionBy(\"typeId\").orderBy(\"account\")))\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndf5 = df4.filter(col(\"userId\") == 'Sample001_MM1')"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df5.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df5.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df = spark.read.text('/FileStore/tables/Beckman_Sample.txt')"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df.show(2)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql.window import *\nfrom pyspark.sql.functions import row_number\ndf4 = df.withColumn(\"row_num\", row_number())\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["df4.printSchema()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql.functions import *\n#df5 = df4[\"value\"].like(\"%Sample001_MM1%\")\ndf5 = df4.filter(df.value.like('%Sample001_MM1%'))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["df5.count()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["df = spark.read.text('/FileStore/tables/Beckman_Sample.txt')\nfrom pyspark.sql.window import *\nfrom pyspark.sql.functions import row_number\n#df4 = df.withColumn(\"row_num\", row_number())\ndf4 = df.filter(df.value.contains('Sample001_MM1')).alias(\"count1\").withColumn(\"row_num\", row_number().over(Window.partitionBy(\"value\").orderBy(\"value\")))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["df4.collect()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"name":"checkline","notebookId":4271020645799337},"nbformat":4,"nbformat_minor":0}
