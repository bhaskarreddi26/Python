<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>CVSML - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta name="robots" content="nofollow">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/bc1c85971ad864da65b19d4f80abe81e111e7bfd8db9cac8f69dc1d94b874689/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/bc1c85971ad864da65b19d4f80abe81e111e7bfd8db9cac8f69dc1d94b874689/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/bc1c85971ad864da65b19d4f80abe81e111e7bfd8db9cac8f69dc1d94b874689/img/favicon.ico"/>
<script>window.settings = {"enableNotebookNotifications":true,"enableSshKeyUI":false,"defaultInteractivePricePerDBU":0.4,"enableClusterMetricsUI":true,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","enableJobsPrefetching":true,"workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableReservoirTableUI":false,"enableClearStateFeature":true,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":true,"enableAttachExistingCluster":true,"resetJobListOnConnect":true,"serverlessDefaultSparkVersion":"latest-stable-scala2.11","maxCustomTags":45,"serverlessDefaultMaxWorkers":20,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":14,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":0,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":6144,"is_hidden":false,"category":"Community Edition","num_cores":0.88,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":false}],"default_node_type_id":"dev-tier-node"},"sqlAclsDisabledMap":{"spark.databricks.acl.enabled":"false","spark.databricks.acl.sqlOnly":"false"},"enableDatabaseSupportClusterChoice":true,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"serverlessClusterProductName":"Serverless Pool","showS3TableImportOption":true,"maxEbsVolumesPerInstance":10,"enableRStudioUI":false,"isAdmin":true,"deltaProcessingBatchSize":1000,"timerUpdateQueueLength":100,"sqlAclsEnabledMap":{"spark.databricks.acl.enabled":"true","spark.databricks.acl.sqlOnly":"true"},"enableLargeResultDownload":true,"maxElasticDiskCapacityGB":5000,"serverlessDefaultMinWorkers":2,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableCustomSpotPricingUIByTier":false,"serverlessClustersEnabled":false,"enableSentryLogging":false,"enableFindAndReplace":true,"disallowUrlImportExceptFromDocs":false,"defaultStandardClusterModel":{"cluster_name":"","node_type_id":"dev-tier-node","spark_version":"3.5.x-scala2.11","num_workers":0,"aws_attributes":{"first_on_demand":0,"availability":"ON_DEMAND","zone_id":"us-west-2c","spot_bid_price_percent":100},"autotermination_minutes":120,"default_tags":{"Vendor":"Databricks","Creator":"bhaskar.reddi@gmail.com","ClusterName":null,"ClusterId":"<Generated after creation>"}},"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableBitbucketCloud":true,"createTableInNotebookS3Link":{"url":"https://docs.databricks.com/_static/notebooks/data-import/s3.html","displayName":"S3","workspaceFileName":"S3 Example"},"sanitizeHtmlResult":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"enableNewClustersCreate":true,"clusters":true,"allowRunOnPendingClusters":true,"useAutoscalingByDefault":false,"enableAzureToolbar":false,"fileStoreBase":"FileStore","enableEmailInAzure":false,"enableRLibraries":true,"enableTableAclsConfig":false,"enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":true,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"checkBeforeAddingAadUser":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"createTableInNotebookDBFSLink":{"url":"https://docs.databricks.com/_static/notebooks/data-import/dbfs.html","displayName":"DBFS","workspaceFileName":"DBFS Example"},"perClusterAutoterminationEnabled":false,"enableNotebookCommandNumbers":true,"allowStyleInSanitizedHtml":true,"sparkVersions":[{"key":"1.6.3-db2-hadoop2-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-aba860a0ffce4f3471fb14aefdcb1d768ac66a53a5ad884c48745ef98aeb9d67","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"3.3.x-gpu-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, GPU, Scala 2.11)","packageLabel":"spark-image-86b4917bb6586289ca64e65f64fd23678c297274be6cd6aa6aa01d7b91fed29c","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.1.1-db5-scala2.11","displayName":"Spark 2.1.1-db5 (Scala 2.11)","packageLabel":"spark-image-08d9fc1551087e0876236f19640c4a83116b1649f15137427d21c9056656e80e","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.3.x-scala2.10","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d7df74e188103a4093ff4467dbf0d32886366c984097f6997e0cd87d0f6b2fa5","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1, deprecated)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.2.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.1.1-db6-scala2.10","displayName":"Spark 2.1.1-db6 (Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"4.0.x-scala2.11","displayName":"4.0 beta (Scala 2.11)","packageLabel":"spark-image-806e2bf6ff456b3832d8f4a8da22564cd3dcb6bbb7da2a910fbdc95597ac2fe6","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION","SUPPORTS_TABLE_ACLS"]},{"key":"2.1.x-gpu-scala2.11","displayName":"Spark 2.1 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-d613235f93e0f29838beb2079a958c02a192ed67a502192bc67a8a5f2fb37f35","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"latest-stable-gpu-scala2.11","displayName":"Latest stable (3.5 LTS, GPU, Scala 2.11)","packageLabel":"spark-image-71e2fd18fdbbb732d6adec03e171846687b6ec85a572e5931e8a9ed9b62e7c32","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.4.x-scala2.11","displayName":"3.4 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-a5615cb1adf0d2305f2b93188c6720174ec3e782d100fcbfa96ff870392861df","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.2.x-scala2.10","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-557788bea0eea16bbf7a8ba13ace07e64dd7fc86270bd5cea086097fe886431f","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"latest-experimental-scala2.10","displayName":"Latest experimental (Scala 2.10)","packageLabel":"spark-image-ec81b6840af02ee2321dd8dfe2587437bbcddf024d4ae287f326a98fac406a6c","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"4.0.x-gpu-scala2.11","displayName":"4.0 beta (GPU, Scala 2.11)","packageLabel":"spark-image-26215c96fc1e7eb1a4657d345beb878d79cea7de1363a3ea12a2904dd17eaa18","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.1.1-db4-scala2.11","displayName":"Spark 2.1.1-db4 (Scala 2.11)","packageLabel":"spark-image-52bca0ca866e3f4243d3820a783abf3b9b3b553edf234abef14b892657ceaca9","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"latest-rc-scala2.11","displayName":"Latest RC (4.0, Scala 2.11)","packageLabel":"spark-image-806e2bf6ff456b3832d8f4a8da22564cd3dcb6bbb7da2a910fbdc95597ac2fe6","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION","SUPPORTS_TABLE_ACLS"]},{"key":"latest-stable-scala2.11","displayName":"Latest stable (3.5 LTS, Scala 2.11)","packageLabel":"spark-image-4f5f9fb3a7177ac43f84a20f819e8ad76833e356707ed0b79812a2f837ac0a06","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION","SUPPORTS_TABLE_ACLS"]},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.0.2-db4-scala2.11","displayName":"Spark 2.0.2-db4 (Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-968b89f1d0ec32e1ee4dacd04838cae25ef44370a441224177a37980d539d83a","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"next-major-version-scala2.11","displayName":"Next major version (4.0 snapshot, Scala 2.11)","packageLabel":"spark-image-d6dcf1e5cebb0329df081e7ecd2f3dc93ab96106049cdc80979bb27197d240c1","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION","SUPPORTS_TABLE_ACLS"]},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"1.6.3-db2-hadoop1-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-14112ea0645bea94333a571a150819ce85573cf5541167d905b7e6588645cf3b","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"3.5.x-scala2.10","displayName":"3.5 LTS (includes Apache Spark 2.2.1, Scala 2.10)","packageLabel":"spark-image-fff24a42ff1d42290166196e45860557b34d4af8a5e6e794857bd999865393fb","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION","SUPPORTS_TABLE_ACLS"]},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.1.1-db4-scala2.10","displayName":"Spark 2.1.1-db4 (Scala 2.10)","packageLabel":"spark-image-c7c0224de396cd1563addc1ae4bca6ba823780b6babe6c3729ddf73008f29ba4","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"latest-rc-scala2.10","displayName":"Latest RC (Scala 2.10)","packageLabel":"spark-image-ec81b6840af02ee2321dd8dfe2587437bbcddf024d4ae287f326a98fac406a6c","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"latest-stable-scala2.10","displayName":"Latest stable (3.5 LTS, Scala 2.10)","packageLabel":"spark-image-fff24a42ff1d42290166196e45860557b34d4af8a5e6e794857bd999865393fb","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION","SUPPORTS_TABLE_ACLS"]},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.2-db4-scala2.10","displayName":"Spark 2.0.2-db4 (Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.1.1-db5-scala2.10","displayName":"Spark 2.1.1-db5 (Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"3.4.x-gpu-scala2.11","displayName":"3.4 (includes Apache Spark 2.2.0, GPU, Scala 2.11)","packageLabel":"spark-image-613a129fcaa93423a4de06407c9f93e341ed5c6b02d69179d2703c8bb47e2b99","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1, deprecated)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"latest-experimental-gpu-scala2.11","displayName":"Latest experimental (4.1 snapshot, GPU, Scala 2.11)","packageLabel":"spark-image-dd98d38d8ee4c19720515230e2b52f26f492d0c94b32c9406f1e7a3f259c1779","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.2.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"3.0.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"3.1.x-scala2.11","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-241fa8b78ee6343242b1756b18076270894385ff40a81172a6fb5eadf66155d3","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.1.0-db3-scala2.10","displayName":"Spark 2.1.0-db3 (Scala 2.10)","packageLabel":"spark-image-25a17d070af155f10c4232dcc6248e36a2eb48c24f8d4fc00f34041b86bd1626","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"3.1.x-scala2.10","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-7efac6b9a8f2da59cb4f6d0caac46cfcb3f1ebf64c8073498c42d0360f846714","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.3.x-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-0badc3d8dfc8cddd55795d02c0b31c76330cfe687d588414f91278197fbc9416","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"next-major-version-gpu-scala2.11","displayName":"Next major version (4.0 snapshot, GPU, Scala 2.11)","packageLabel":"spark-image-fe0b3ec2ae0caa7f2187ebf3a63f7003c29e7d4902f57140fca733452006f27d","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.5.x-gpu-scala2.11","displayName":"3.5 LTS (includes Apache Spark 2.2.1, GPU, Scala 2.11)","packageLabel":"spark-image-71e2fd18fdbbb732d6adec03e171846687b6ec85a572e5931e8a9ed9b62e7c32","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1, deprecated)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.1.1-db6-scala2.11","displayName":"Spark 2.1.1-db6 (Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"3.5.x-scala2.11","displayName":"3.5 LTS (includes Apache Spark 2.2.1, Scala 2.11)","packageLabel":"spark-image-4f5f9fb3a7177ac43f84a20f819e8ad76833e356707ed0b79812a2f837ac0a06","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION","SUPPORTS_TABLE_ACLS"]},{"key":"latest-experimental-scala2.11","displayName":"Latest experimental (4.1 snapshot, Scala 2.11)","packageLabel":"spark-image-98ea8c54b4bd5547e01ab9e32730370749d1f5a25d92904e134b74cc4931e3ef","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION","SUPPORTS_TABLE_ACLS"]},{"key":"3.2.x-scala2.11","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-5537926238bc55cb6cd76ee0f0789511349abead3781c4780721a845f34b5d4e","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":[]},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"3.0.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false,"capabilities":[]},{"key":"2.1.0-db3-scala2.11","displayName":"Spark 2.1.0-db3 (Scala 2.11)","packageLabel":"spark-image-ccbc6b73f158e2001fc1fb8c827bfdde425d8bd6d65cb7b3269784c28bb72c16","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]},{"key":"latest-rc-gpu-scala2.11","displayName":"Latest RC (4.0 GPU, Scala 2.11)","packageLabel":"spark-image-26215c96fc1e7eb1a4657d345beb878d79cea7de1363a3ea12a2904dd17eaa18","upgradable":true,"deprecated":false,"customerVisible":false,"capabilities":[]},{"key":"3.4.x-scala2.10","displayName":"3.4 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-b768d65de82a89fbfabff8ec1d2f279ced527c0ec05e83c3ae0c206d2e97edc0","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION"]}],"enablePresentationMode":false,"enableClearStateAndRunAll":true,"enableTableAclsByTier":false,"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"jobsUnreachableThresholdMillis":60000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"createTableInNotebookImportedFileLink":{"url":"https://docs.databricks.com/_static/notebooks/data-import/imported-file.html","displayName":"Imported File","workspaceFileName":"Imported File Example"},"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","tableAclsDisabledMap":{"spark.databricks.acl.dfAclsEnabled":"false"},"driverStdoutFilePrefix":"stdout","showDbuPricing":true,"databricksDocsBaseHostname":"docs.databricks.com","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"i3.4xlarge":4,"class-node":1,"m4.2xlarge":1.5,"r4.xlarge":1,"m4.4xlarge":3,"Standard_DS5_v2":3,"Standard_D2s_v3":0.5,"Standard_DS14":4,"r4.16xlarge":16,"Standard_DS11":0.5,"Standard_D64s_v3":12,"p2.8xlarge":16,"m4.10xlarge":8,"Standard_D8s_v3":1.5,"Standard_E32s_v3":8,"Standard_DS3":0.75,"Standard_DS2_v2":0.5,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"Standard_L8s":2,"Standard_E4s_v3":1,"Standard_D3_v2":0.75,"Standard_DS15_v2":5,"Standard_D16s_v3":3,"Standard_D5_v2":3,"Standard_E8s_v3":2,"c3.8xlarge":4,"Standard_E2s_v3":0.5,"Standard_DS3_v2":0.75,"r3.4xlarge":4,"Standard_DS4":1.5,"i2.4xlarge":6,"m4.xlarge":0.75,"r4.8xlarge":8,"Standard_H16":4,"Standard_DS14_v2":4,"r4.large":0.5,"Standard_DS12":1,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"i3.large":0.75,"memory-optimized":1,"m4.large":0.4,"Standard_F4s":0.5,"p2.16xlarge":24,"i3.8xlarge":8,"Standard_D32s_v3":6,"i3.16xlarge":16,"Standard_DS12_v2":1,"Standard_L32s":8,"Standard_D4s_v3":0.75,"Standard_DS13":2,"Standard_DS11_v2":0.5,"Standard_DS13_v2":2,"c3.2xlarge":1,"Standard_L4s":1,"Standard_F16s":2,"c4.2xlarge":1,"Standard_L16s":4,"i2.xlarge":1.5,"Standard_DS2":0.5,"compute-optimized":1,"c4.4xlarge":2,"Standard_D2_v2":0.5,"i3.2xlarge":2,"Standard_E16s_v3":4,"Standard_F8s":1,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":12,"Standard_DS4_v2":1.5,"c4.8xlarge":4,"i3.xlarge":1,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"tableFilesBaseFolder":"/tables","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableClusterAppsUIOnServerless":false,"enableEBSVolumesUI":false,"homePageWelcomeMessage":"Welcome to ","metastoreServiceRowLimit":1000000,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.64.930","accountsLimit":3,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"useAADLogin":false,"enableStructuredStreamingNbOptimizations":true,"enableNotebookGitBranching":true,"local":false,"enableNotebookLazyRenderWrapper":false,"enableClusterAutoScalingForJobs":true,"enableStrongPassword":false,"showReleaseNote":true,"displayDefaultContainerMemoryGB":6,"broadenedEditPermission":false,"enableNotebookCommandMode":true,"disableS3TableImport":false,"enableArrayParamsEdit":true,"deploymentMode":"production","useSpotForWorkers":true,"removePasswordInAccountSettings":false,"preferStartTerminatedCluster":false,"enableUserInviteWorkflow":true,"createTableConnectorOptionLinks":[{"url":"https://docs.databricks.com/_static/notebooks/redshift.html","displayName":"Amazon Redshift","workspaceFileName":"Amazon Redshift Example"},{"url":"https://docs.databricks.com/_static/notebooks/structured-streaming-kinesis.html","displayName":"Amazon Kinesis","workspaceFileName":"Amazon Kinesis Example"},{"url":"https://docs.databricks.com/_static/notebooks/data-import/jdbc.html","displayName":"JDBC","workspaceFileName":"JDBC Example"},{"url":"https://docs.databricks.com/_static/notebooks/cassandra.html","displayName":"Cassandra","workspaceFileName":"Cassandra Example"},{"url":"https://docs.databricks.com/_static/notebooks/structured-streaming-etl-kafka.html","displayName":"Kafka","workspaceFileName":"Kafka Example"},{"url":"https://docs.databricks.com/_static/notebooks/redis.html","displayName":"Redis","workspaceFileName":"Redis Example"},{"url":"https://docs.databricks.com/_static/notebooks/elasticsearch.html","displayName":"Elasticsearch","workspaceFileName":"Elasticsearch Example"}],"enableStaticNotebooks":true,"sandboxForUrlSandboxFrame":"allow-scripts allow-popups allow-popups-to-escape-sandbox allow-forms","enableCssTransitions":true,"serverlessEnableElasticDisk":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterEdit":true,"enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableSshKeyUIByTier":false,"enableCreateClusterOnAttach":true,"defaultAutomatedPricePerDBU":0.2,"enableNotebookGitVersioning":true,"defaultMinWorkers":2,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"defaultMaxWorkers":8,"enableWorkspaceAclsConfig":false,"serverlessRunPythonAsLowPrivilegeUser":false,"dropzoneMaxFileSize":2047,"enableNewClustersList":true,"enableNewDashboardViews":true,"enableJobListPermissionFilter":false,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"3.5.x-scala2.11","displayName":"3.5 LTS (includes Apache Spark 2.2.1, Scala 2.11)","packageLabel":"spark-image-4f5f9fb3a7177ac43f84a20f819e8ad76833e356707ed0b79812a2f837ac0a06","upgradable":true,"deprecated":false,"customerVisible":true,"capabilities":["SUPPORTS_END_TO_END_ENCRYPTION","SUPPORTS_TABLE_ACLS"]},"deprecatedEnableStructuredDataAcls":false,"enableCustomSpotPricing":false,"enableMountAclsConfig":false,"defaultAutoterminationMin":120,"useDevTierHomePage":true,"disableExportNotebook":false,"enableClusterClone":true,"enableNotebookLineNumbers":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableNotebookDatasetInfoView":true,"defaultTagKeys":{"CLUSTER_NAME":"ClusterName","VENDOR":"Vendor","CLUSTER_TYPE":"ResourceClass","CREATOR":"Creator","CLUSTER_ID":"ClusterId"},"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","azurePortalLink":"https://portal.azure.com","cloud":"AWS","customSparkVersionPrefix":"custom:","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","mavenCentralSearchEndpoint":"http://search.maven.org/solrsearch/select","defaultServerlessClusterModel":{"cluster_name":"","node_type_id":"i3.2xlarge","spark_version":"latest-stable-scala2.11","num_workers":null,"enable_jdbc_auto_start":true,"custom_tags":{"ResourceClass":"Serverless"},"autoscale":{"min_workers":2,"max_workers":20},"spark_conf":{"spark.databricks.cluster.profile":"serverless","spark.databricks.repl.allowedLanguages":"sql,python","spark.databricks.acl.enabled":"false","spark.databricks.acl.sqlOnly":"false"},"aws_attributes":{"ebs_volume_count":null,"availability":"ON_DEMAND","first_on_demand":1,"ebs_volume_type":null,"spot_bid_price_percent":100,"zone_id":"us-west-2c","ebs_volume_size":null},"autotermination_minutes":0,"enable_elastic_disk":false,"default_tags":{"Vendor":"Databricks","Creator":"bhaskar.reddi@gmail.com","ClusterName":null,"ClusterId":"<Generated after creation>"}},"enableOrgSwitcherUI":true,"bitbucketCloudBaseApiV2Url":"https://api.bitbucket.org/2.0","clustersLimit":1,"enableJdbcImport":true,"enableClusterAppsUIOnNormalClusters":false,"enableElasticDisk":false,"logfiles":"logfiles/","enableRelativeNotebookLinks":true,"enableMultiSelect":true,"homePageLogo":"login/databricks_logoTM_rgb_TM.svg","enableWebappSharding":true,"enableNotebookParamsEdit":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"separateTableForJobClusters":true,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"showVersion":true,"serverlessClustersByDefault":false,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"","clusterTagReservedPrefixes":[],"tableAclsEnabledMap":{"spark.databricks.acl.dfAclsEnabled":"true"},"showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","databricksDocsNotebookPathPrefix":"^https://docs\\.databricks\\.com/_static/notebooks/.+$","serverlessAttachEbsVolumesByDefault":false,"enableTokensConfig":false,"allowFeedbackForumAccess":true,"enableImportFromUrl":true,"allowDisplayHtmlByUrl":true,"enableTokens":false,"enableMiniClusters":true,"enableNewJobList":true,"enableDebugUI":false,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"loginLogo":"/login/databricks_logoTM_rgb_TM.svg","useStandardTierUpgradeTooltips":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/bc1c85971ad864da65b19d4f80abe81e111e7bfd8db9cac8f69dc1d94b874689/","enableSpotClusterType":true,"enableSparkPackages":true,"checkAadUserInWorkspaceTenant":false,"dynamicSparkVersions":true,"useIframeForHtmlResult":false,"enableClusterTagsUIByTier":false,"enableNotebookHistoryUI":true,"addWhitespaceAfterLastNotebookCell":true,"enableClusterLoggingUI":true,"enableDatabaseDropdownInTableUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"enableClusterStart":false,"maxImportFileVersion":5,"enableEBSVolumesUIByTier":false,"enableTableAclService":true,"removeSubCommandCodeWhenExport":true,"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","maxAutoterminationMinutes":10000,"showResultsFromExternalSearchEngine":true,"autoterminateClustersByDefault":true,"notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"showForgotPasswordLink":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"minAutoterminationMinutes":10,"accounts":true,"useOnDemandClustersByDefault":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"enableAutoCreateUserUI":true,"defaultCoresPerContainer":4,"showTerminationReason":true,"enableNewClustersGet":true,"showPricePerDBU":false,"showSqlProxyUI":true,"enableNotebookErrorHighlighting":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":1564452080561839,"name":"CVSML","language":"python","commands":[{"version":"CommandV1","origId":1564452080561840,"guid":"2f41e7c3-4cd0-4cfc-a16f-da9ffc755a9f","subtype":"command","commandType":"auto","position":1.0,"command":"DF = spark.read.csv('/FileStore/tables/UsersImportMinimumSample.csv', header=True, inferSchema=True)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"DF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"SRM_SaaS_ES","nullable":true,"type":"string"},{"metadata":{},"name":"MXPERUSERInterface","nullable":true,"type":"string"},{"metadata":{},"name":"Add","nullable":true,"type":"string"},{"metadata":{},"name":"EN","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":"<span class=\"ansired\">TypeError</span>: csv() got an unexpected keyword argument &apos;inferschema&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1564452080561840&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>DF <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>csv<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;/FileStore/tables/UsersImportMinimumSample.csv&apos;</span><span class=\"ansiyellow\">,</span> header<span class=\"ansiyellow\">=</span>True<span class=\"ansiyellow\">,</span> inferschema<span class=\"ansiyellow\">=</span>True<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: csv() got an unexpected keyword argument &apos;inferschema&apos;</div>","workflows":[],"startTime":1514885668041,"submitTime":1514885512664,"finishTime":1514885671830,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f1d61b3f-955c-4208-a469-f569962407b4"},{"version":"CommandV1","origId":1564452080561843,"guid":"6ebec79e-09e6-4ba4-b1c5-ab81c20271da","subtype":"command","commandType":"auto","position":2.0,"command":"DF.printSchema()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">root\n |-- SRM_SaaS_ES: string (nullable = true)\n |-- MXPERUSERInterface: string (nullable = true)\n |-- Add: string (nullable = true)\n |-- EN: string (nullable = true)\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;df&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1564452080561843&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>df<span class=\"ansiyellow\">.</span>printSchema<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;df&apos; is not defined</div>","workflows":[],"startTime":1511860838118,"submitTime":1511860838110,"finishTime":1511860838192,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2a904e8b-75ee-406e-a3d3-154b88697c2f"},{"version":"CommandV1","origId":1564452080561845,"guid":"48aaa877-690e-42a9-b7b2-81dff672dc02","subtype":"command","commandType":"auto","position":3.0,"command":"DF.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-----------+------------------+--------+-------------+\n|SRM_SaaS_ES|MXPERUSERInterface|     Add|           EN|\n+-----------+------------------+--------+-------------+\n|    LOGINID|          PERSONID|  USERID|PASSWORDINPUT|\n|   manoel01|          manoel01|manoel01|    manoelpwd|\n+-----------+------------------+--------+-------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511860881089,"submitTime":1511860881073,"finishTime":1511860881469,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4cbc093b-3c8d-4456-b643-17e8e74bde23"},{"version":"CommandV1","origId":1564452080561846,"guid":"535adf92-e0e9-440c-9e97-51ca9a3d1dc2","subtype":"command","commandType":"auto","position":4.0,"command":"import pyspark.ml.feature as ft\ntransfer_srm = ft.StringIndexer(inputCol='SRM_SaaS_ES', outputCol='SRM_SaaS_ES_en')\ntransfer_mxp = ft.StringIndexer(inputCol='MXPERUSERInterface', outputCol='MXPERUSERInterface_en')\ntransfer_Add = ft.StringIndexer(inputCol='Add', outputCol='Add_en')\ntransfer_EN = ft.StringIndexer(inputCol='EN', outputCol='EN_en')","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511861269987,"submitTime":1511861269978,"finishTime":1511861270113,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dc9dc486-4c9b-40ee-ab14-f36e55803401"},{"version":"CommandV1","origId":1564452080561847,"guid":"23236530-8318-453e-a009-718628a1aa18","subtype":"command","commandType":"auto","position":5.0,"command":"from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.stat import Correlation\n\ndata = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\ndf = spark.createDataFrame(data, [\"features\"])\n\nr1 = Correlation.corr(df, \"features\").head()\nprint(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n\nr2 = Correlation.corr(df, \"features\", \"spearman\").head()\nprint(\"Spearman correlation matrix:\\n\" + str(r2[0]))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Pearson correlation matrix:\nDenseMatrix([[ 1.        ,  0.05564149,         nan,  0.40047142],\n             [ 0.05564149,  1.        ,         nan,  0.91359586],\n             [        nan,         nan,  1.        ,         nan],\n             [ 0.40047142,  0.91359586,         nan,  1.        ]])\nSpearman correlation matrix:\nDenseMatrix([[ 1.        ,  0.10540926,         nan,  0.4       ],\n             [ 0.10540926,  1.        ,         nan,  0.9486833 ],\n             [        nan,         nan,  1.        ,         nan],\n             [ 0.4       ,  0.9486833 ,         nan,  1.        ]])\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511866710337,"submitTime":1511866710329,"finishTime":1511866713768,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d9397b06-76ba-43c9-9865-dc90d2e737b6"},{"version":"CommandV1","origId":1564452080561848,"guid":"25d5ea8a-fe92-4a24-83d8-ca6aff57d42f","subtype":"command","commandType":"auto","position":6.0,"command":"from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.stat import ChiSquareTest\n\ndata = [(0.0, Vectors.dense(0.5, 10.0)),\n        (0.0, Vectors.dense(1.5, 20.0)),\n        (1.0, Vectors.dense(1.5, 30.0)),\n        (0.0, Vectors.dense(3.5, 30.0)),\n        (0.0, Vectors.dense(3.5, 40.0)),\n        (1.0, Vectors.dense(3.5, 40.0))]\ndf = spark.createDataFrame(data, [\"label\", \"features\"])\n\nr = ChiSquareTest.test(df, \"features\", \"label\").head()\nprint(\"pValues: \" + str(r.pValues))\nprint(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\nprint(\"statistics: \" + str(r.statistics))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">pValues: [0.687289278791,0.682270330336]\ndegreesOfFreedom: [2, 3]\nstatistics: [0.75,1.5]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"label","nullable":true,"type":"double"},{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511866869340,"submitTime":1511866869332,"finishTime":1511866870120,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fda3f2a4-7567-4058-8d31-7a205efa0401"},{"version":"CommandV1","origId":1564452080561849,"guid":"6e581b99-a942-4c2a-a796-5d32c354dc60","subtype":"command","commandType":"auto","position":7.0,"command":"from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentenceData = spark.createDataFrame([\n    (0.0, \"Hi I heard about Spark\"),\n    (0.0, \"I wish Java could use case classes\"),\n    (1.0, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData)\n\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(wordsData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)\n\nrescaledData.select(\"label\", \"features\").show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(20,[0,5,9,17],[0...|\n|  0.0|(20,[2,7,9,13,15]...|\n|  1.0|(20,[4,6,13,15,18...|\n+-----+--------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"sentenceData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"label","nullable":true,"type":"double"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"wordsData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"label","nullable":true,"type":"double"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}}],"type":"struct"},"tableIdentifier":null},{"name":"featurizedData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"label","nullable":true,"type":"double"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}},{"metadata":{"ml_attr":{"num_attrs":20}},"name":"rawFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null},{"name":"rescaledData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"label","nullable":true,"type":"double"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}},{"metadata":{"ml_attr":{"num_attrs":20}},"name":"rawFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511870677112,"submitTime":1511870677100,"finishTime":1511870678109,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9a6a600d-b62d-446c-9f53-ce0aaedd1372"},{"version":"CommandV1","origId":1564452080561850,"guid":"216088c6-27c8-4ac6-9493-7b0007b0c5f8","subtype":"command","commandType":"auto","position":8.0,"command":"from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentenceData = spark.createDataFrame([\n    (0.0, \"Hi I heard about Spark\"),\n    (0.0, \"I wish Java could use case classes\"),\n    (1.0, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"sentenceData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"label","nullable":true,"type":"double"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"wordsData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"label","nullable":true,"type":"double"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511870710848,"submitTime":1511870710837,"finishTime":1511870710975,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9c43b8a8-3ac1-4d4c-9edc-21431fa2b26f"},{"version":"CommandV1","origId":1564452080561851,"guid":"678400ad-d720-4f3f-8005-08f4d6dbfefe","subtype":"command","commandType":"auto","position":9.0,"command":"wordsData.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-----+--------------------+--------------------+\n|label|            sentence|               words|\n+-----+--------------------+--------------------+\n|  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n|  0.0|I wish Java could...|[i, wish, java, c...|\n|  1.0|Logistic regressi...|[logistic, regres...|\n+-----+--------------------+--------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511870730801,"submitTime":1511870730791,"finishTime":1511870731130,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"03b94fc5-ac84-4fd8-9cb0-6513ba67418f"},{"version":"CommandV1","origId":1564452080561852,"guid":"16073499-97ac-49da-86b8-bd59a1e389f7","subtype":"command","commandType":"auto","position":10.0,"command":"hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\nfeaturizedData = hashingTF.transform(wordsData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"featurizedData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"label","nullable":true,"type":"double"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}},{"metadata":{"ml_attr":{"num_attrs":10}},"name":"rawFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511874483090,"submitTime":1511874483082,"finishTime":1511874483170,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"41d6a240-3570-4df6-80ba-018ed0962ed2"},{"version":"CommandV1","origId":1564452080561853,"guid":"82664b71-96ec-4e67-95f5-8f81c347ef76","subtype":"command","commandType":"auto","position":11.0,"command":"featurizedData.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-----+--------------------+--------------------+--------------------+\n|label|            sentence|               words|         rawFeatures|\n+-----+--------------------+--------------------+--------------------+\n|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(10,[0,5,7,9],[1....|\n|  0.0|I wish Java could...|[i, wish, java, c...|(10,[2,3,5,7,9],[...|\n|  1.0|Logistic regressi...|[logistic, regres...|(10,[3,4,5,6,8],[...|\n+-----+--------------------+--------------------+--------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511874488000,"submitTime":1511874487988,"finishTime":1511874488702,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ab051b63-bcd0-48f6-9882-1a23c03a9837"},{"version":"CommandV1","origId":1564452080561854,"guid":"125fd0b8-269c-4315-902d-89fc18284c76","subtype":"command","commandType":"auto","position":12.0,"command":"idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"rescaledData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"label","nullable":true,"type":"double"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}},{"metadata":{"ml_attr":{"num_attrs":10}},"name":"rawFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511875540563,"submitTime":1511875540554,"finishTime":1511875540840,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6aa3f6f1-501a-4531-90b2-50653ef871ea"},{"version":"CommandV1","origId":1564452080561855,"guid":"37165068-adf2-4a14-83de-d84939e7c481","subtype":"command","commandType":"auto","position":13.0,"command":"rescaledData.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-----+--------------------+--------------------+--------------------+--------------------+\n|label|            sentence|               words|         rawFeatures|            features|\n+-----+--------------------+--------------------+--------------------+--------------------+\n|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(10,[0,5,7,9],[1....|(10,[0,5,7,9],[0....|\n|  0.0|I wish Java could...|[i, wish, java, c...|(10,[2,3,5,7,9],[...|(10,[2,3,5,7,9],[...|\n|  1.0|Logistic regressi...|[logistic, regres...|(10,[3,4,5,6,8],[...|(10,[3,4,5,6,8],[...|\n+-----+--------------------+--------------------+--------------------+--------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511875544057,"submitTime":1511875544046,"finishTime":1511875544589,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6d4bfb70-8b69-482b-9041-506191d71e89"},{"version":"CommandV1","origId":1564452080561856,"guid":"8d6d1902-89f2-45ba-8b37-5b097a8e90b7","subtype":"command","commandType":"auto","position":14.0,"command":"#Word2Vec is an Estimator which takes sequences of words representing documents and trains a Word2VecModel. The model maps each word to a unique\n#fixed-size vector.\nfrom pyspark.ml.feature import Word2Vec\n\n# Input data: Each row is a bag of words from a sentence or document.\ndocumentDF = spark.createDataFrame([\n    (\"Hi I heard about Spark\".split(\" \"), ),\n    (\"I wish Java could use case classes\".split(\" \"), ),\n    (\"Logistic regression models are neat\".split(\" \"), )\n], [\"text\"])\n\n# Learn a mapping from words to Vectors.\nword2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\nmodel = word2Vec.fit(documentDF)\n\nresult = model.transform(documentDF)\nfor row in result.collect():\n    text, vector = row\n    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Text: [Hi, I, heard, about, Spark] =&gt; \nVector: [0.0135332792997,-0.011096050078,0.0506678894162]\n\nText: [I, wish, Java, could, use, case, classes] =&gt; \nVector: [0.0376478566655,0.0210807355387,0.0403019455927]\n\nText: [Logistic, regression, models, are, neat] =&gt; \nVector: [0.0177810560912,-0.0559235086665,-0.0178805116564]\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"documentDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"text","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}}],"type":"struct"},"tableIdentifier":null},{"name":"result","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"text","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}},{"metadata":{},"name":"result","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511934662827,"submitTime":1511934662827,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"02cb3e56-5de7-4b99-8495-7493e60f8152"},{"version":"CommandV1","origId":1564452080561857,"guid":"5ac2e4c8-7145-4a4f-8283-32c8b584b5c0","subtype":"command","commandType":"auto","position":15.0,"command":"#CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. \nfrom pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a d e\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=4, minDF=2.0)\n\nmodel = cv.fit(df)\n\nresult = model.transform(df)\nresult.show(truncate=False)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+---+---------------------+-------------------------+\n|id |words                |features                 |\n+---+---------------------+-------------------------+\n|0  |[a, b, c]            |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a, d, e]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------------+-------------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}}],"type":"struct"},"tableIdentifier":null},{"name":"result","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}},{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511875838063,"submitTime":1511875838054,"finishTime":1511875838644,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"06553369-b063-4aca-955f-0d53fe0d29ef"},{"version":"CommandV1","origId":1564452080561858,"guid":"ea2908d3-9e40-408d-89a4-3d665a51bc5d","subtype":"command","commandType":"auto","position":16.0,"command":"#Feature Transformers\n#Tokenizer take a input as a sentence and output as a breaking it into individual words\n#RegexTokenizer is taking input as a sentence and output as a words. It's more advanced  tokenization based on regular expression (regex) matching. By #default, the parameter pattern (regex, default: \"\\\\s+\") is used as delimiters to split the input text. \nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType\n\nsentenceDataFrame = spark.createDataFrame([\n    (0, \"Hi I heard about Spark\"),\n    (1, \"I wish Java could use case classes\"),\n    (2, \"Logistic,regression,models,are,neat,reddy\")\n], [\"id\", \"sentence\"])\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n# alternatively, pattern=\"\\\\w+\", gaps(False)\n\ncountTokens = udf(lambda words: len(words), IntegerType())\n\ntokenized = tokenizer.transform(sentenceDataFrame)\ntokenized.select(\"sentence\", \"words\")\\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\nregexTokenized = regexTokenizer.transform(sentenceDataFrame)\nregexTokenized.select(\"sentence\", \"words\") \\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+-----------------------------------------+-------------------------------------------+------+\n|sentence                                 |words                                      |tokens|\n+-----------------------------------------+-------------------------------------------+------+\n|Hi I heard about Spark                   |[hi, i, heard, about, spark]               |5     |\n|I wish Java could use case classes       |[i, wish, java, could, use, case, classes] |7     |\n|Logistic,regression,models,are,neat,reddy|[logistic,regression,models,are,neat,reddy]|1     |\n+-----------------------------------------+-------------------------------------------+------+\n\n+-----------------------------------------+------------------------------------------------+------+\n|sentence                                 |words                                           |tokens|\n+-----------------------------------------+------------------------------------------------+------+\n|Hi I heard about Spark                   |[hi, i, heard, about, spark]                    |5     |\n|I wish Java could use case classes       |[i, wish, java, could, use, case, classes]      |7     |\n|Logistic,regression,models,are,neat,reddy|[logistic, regression, models, are, neat, reddy]|6     |\n+-----------------------------------------+------------------------------------------------+------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"sentenceDataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"tokenized","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}}],"type":"struct"},"tableIdentifier":null},{"name":"regexTokenized","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"sentence","nullable":true,"type":"string"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511877404188,"submitTime":1511877404180,"finishTime":1511877405176,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dd1073c0-e82a-44d4-8067-50b67dd23ed3"},{"version":"CommandV1","origId":1564452080561859,"guid":"7cf5b713-2951-4ea5-9353-c81cc6ee726e","subtype":"command","commandType":"auto","position":17.0,"command":"# StopWordRemover is remove the single letter or unnecessary words from the sentence\nfrom pyspark.ml.feature import StopWordsRemover\n\nsentenceData = spark.createDataFrame([\n    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n], [\"id\", \"raw\"])\n\nremover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\nremover.transform(sentenceData).show(truncate=False)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+---+----------------------------+--------------------+\n|id |raw                         |filtered            |\n+---+----------------------------+--------------------+\n|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"sentenceData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"raw","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511877536182,"submitTime":1511877536172,"finishTime":1511877536558,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4c6a5d08-457c-49f2-a618-89d604310072"},{"version":"CommandV1","origId":1564452080561860,"guid":"2b93e5ba-f8af-46f5-aafe-ead65bc199d3","subtype":"command","commandType":"auto","position":18.0,"command":"#NGram taking the words as input and create the n sequence of words\nfrom pyspark.ml.feature import NGram\n\nwordDataFrame = spark.createDataFrame([\n    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n], [\"id\", \"words\"])\n\nngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n\nngramDataFrame = ngram.transform(wordDataFrame)\nngramDataFrame.select(\"ngrams\").show(truncate=False)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+------------------------------------------------------------------+\n|ngrams                                                            |\n+------------------------------------------------------------------+\n|[Hi I, I heard, heard about, about Spark]                         |\n|[I wish, wish Java, Java could, could use, use case, case classes]|\n|[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"wordDataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}}],"type":"struct"},"tableIdentifier":null},{"name":"ngramDataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"words","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}},{"metadata":{},"name":"ngrams","nullable":true,"type":{"containsNull":false,"elementType":"string","type":"array"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511934098027,"submitTime":1511934098027,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a378928a-1164-49cd-8b64-b614c285ab78"},{"version":"CommandV1","origId":1564452080561861,"guid":"27a24e80-4ceb-497f-9e59-2836c0d03855","subtype":"command","commandType":"auto","position":19.0,"command":"#Binarization is the process of thresholding numerical features to binary (0/1) features.\nfrom pyspark.ml.feature import Binarizer\n\ncontinuousDataFrame = spark.createDataFrame([\n    (0, 0.1),\n    (1, 0.8),\n    (2, 0.2),\n    (3, 1.2)\n], [\"id\", \"feature\"])\n\nbinarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n\nbinarizedDataFrame = binarizer.transform(continuousDataFrame)\n\nprint(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\nbinarizedDataFrame.show()\nbinarizer.write().save('/FileStore/tables/reddyb.txt')","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Binarizer output with Threshold = 0.500000\n+---+-------+-----------------+\n| id|feature|binarized_feature|\n+---+-------+-----------------+\n|  0|    0.1|              0.0|\n|  1|    0.8|              1.0|\n|  2|    0.2|              0.0|\n|  3|    1.2|              1.0|\n+---+-------+-----------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"continuousDataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"feature","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"binarizedDataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"feature","nullable":true,"type":"double"},{"metadata":{"ml_attr":{"type":"binary"}},"name":"binarized_feature","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":"<span class=\"ansired\">TypeError</span>: &apos;DataFrameWriter&apos; object is not callable","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1564452080561861&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     15</span> <span class=\"ansigreen\">print</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Binarizer output with Threshold = %f&quot;</span> <span class=\"ansiyellow\">%</span> binarizer<span class=\"ansiyellow\">.</span>getThreshold<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     16</span> binarizedDataFrame<span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 17</span><span class=\"ansiyellow\"> </span>binarizedDataFrame<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>save<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;/FileStore/tables/reddyb.txt&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: &apos;DataFrameWriter&apos; object is not callable</div>","workflows":[],"startTime":1511935491053,"submitTime":1511935491037,"finishTime":1511935493273,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e48b1e93-f59c-4cf1-a747-8b54ee6f473c"},{"version":"CommandV1","origId":1859291325708963,"guid":"c74de2f7-6b63-4f3c-be44-1b049015e7ea","subtype":"command","commandType":"auto","position":20.0,"command":"#PCA trains a model to project vectors to a lower dimensional space of the top k principal components.\nfrom pyspark.ml.feature import PCA\nfrom pyspark.ml.linalg import Vectors\n\ndata = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\ndf = spark.createDataFrame(data, [\"features\"])\nprint df.show()\npca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\nmodel = pca.fit(df)\n\nresult = model.transform(df).select(\"pcaFeatures\")\nresult.show(truncate=False)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+--------------------+\n|            features|\n+--------------------+\n| (5,[1,3],[1.0,7.0])|\n|[2.0,0.0,3.0,4.0,...|\n|[4.0,0.0,0.0,6.0,...|\n+--------------------+\n\nNone\n+-----------------------------------------------------------+\n|pcaFeatures                                                |\n+-----------------------------------------------------------+\n|[1.6485728230883807,-4.013282700516296,-5.51655055421941]  |\n|[-4.645104331781532,-1.1167972663619032,-5.516550554219409]|\n|[-6.428880535676488,-5.337951427775355,-5.51655055421941]  |\n+-----------------------------------------------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null},{"name":"result","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"pcaFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;PCAModel&apos; object has no attribute &apos;take&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1859291325708963&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      9</span> pca <span class=\"ansiyellow\">=</span> PCA<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">,</span> inputCol<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;features&quot;</span><span class=\"ansiyellow\">,</span> outputCol<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;pcaFeatures&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     10</span> model <span class=\"ansiyellow\">=</span> pca<span class=\"ansiyellow\">.</span>fit<span class=\"ansiyellow\">(</span>df<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 11</span><span class=\"ansiyellow\"> </span>model<span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     12</span> result <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>transform<span class=\"ansiyellow\">(</span>df<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;pcaFeatures&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     13</span> result<span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span>truncate<span class=\"ansiyellow\">=</span>False<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;PCAModel&apos; object has no attribute &apos;take&apos;</div>","workflows":[],"startTime":1511936640143,"submitTime":1511936640087,"finishTime":1511936641256,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"69716a60-f938-4833-95ac-e6cb1b2e22e6"},{"version":"CommandV1","origId":1859291325708964,"guid":"4614ccf3-7952-4c6a-9fec-447ef7f035b6","subtype":"command","commandType":"auto","position":21.0,"command":"#PolynomialExpansionis the process of expanding your features into a polynomial space, which is formulated by an n-degree combination of original #dimensions\nfrom pyspark.ml.feature import PolynomialExpansion\nfrom pyspark.ml.linalg import Vectors\n\ndf = spark.createDataFrame([\n    (Vectors.dense([2.0, 1.0]),),\n    (Vectors.dense([0.0, 0.0]),),\n    (Vectors.dense([3.0, -1.0]),)\n], [\"features\"])\n\npolyExpansion = PolynomialExpansion(degree=2, inputCol=\"features\", outputCol=\"polyFeatures\")\npolyDF = polyExpansion.transform(df)\n\npolyDF.show(truncate=False)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+----------+-----------------------+\n|features  |polyFeatures           |\n+----------+-----------------------+\n|[2.0,1.0] |[2.0,4.0,1.0,2.0,1.0]  |\n|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0]  |\n|[3.0,-1.0]|[3.0,9.0,-1.0,-3.0,1.0]|\n+----------+-----------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null},{"name":"polyDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"polyFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511936966383,"submitTime":1511936966368,"finishTime":1511936966717,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a6a3e5cc-7bc7-4d8e-aa6b-b66a1913b5aa"},{"version":"CommandV1","origId":1859291325708965,"guid":"2bab2fcb-c759-4761-84cc-e94af97745fd","subtype":"command","commandType":"auto","position":22.0,"command":"from pyspark.ml.feature import StringIndexer\n\ndf = spark.createDataFrame(\n    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\"), (6, \"d\")],\n    [\"id\", \"category\"])\n\nindexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n\nindexed = indexer.fit(df).transform(df)\nindexed.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+---+--------+-------------+\n| id|category|categoryIndex|\n+---+--------+-------------+\n|  0|       a|          0.0|\n|  1|       b|          2.0|\n|  2|       c|          1.0|\n|  3|       a|          0.0|\n|  4|       a|          0.0|\n|  5|       c|          1.0|\n|  6|       d|          3.0|\n+---+--------+-------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"category","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"indexed","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"category","nullable":true,"type":"string"},{"metadata":{"ml_attr":{"name":"categoryIndex","type":"nominal","vals":["a","c","b","d"]}},"name":"categoryIndex","nullable":false,"type":"double"}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;StringIndexer&apos; object has no attribute &apos;take&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1859291325708965&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      6</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      7</span> indexer <span class=\"ansiyellow\">=</span> StringIndexer<span class=\"ansiyellow\">(</span>inputCol<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;category&quot;</span><span class=\"ansiyellow\">,</span> outputCol<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;categoryIndex&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 8</span><span class=\"ansiyellow\"> </span><span class=\"ansigreen\">print</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;print &apos;</span><span class=\"ansiyellow\">,</span> indexer<span class=\"ansiyellow\">.</span>setHandleInvalid<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;skip&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      9</span> indexed <span class=\"ansiyellow\">=</span> indexer<span class=\"ansiyellow\">.</span>fit<span class=\"ansiyellow\">(</span>df<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>transform<span class=\"ansiyellow\">(</span>df<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     10</span> indexed<span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;StringIndexer&apos; object has no attribute &apos;take&apos;</div>","workflows":[],"startTime":1511937716797,"submitTime":1511937716787,"finishTime":1511937717325,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6c2f5827-c8c4-4edd-bf08-bdb7541bdc3e"},{"version":"CommandV1","origId":1859291325708966,"guid":"4ad26f8d-5f34-4688-8133-2ae483b3019e","subtype":"command","commandType":"auto","position":23.0,"command":"from pyspark.ml.feature import IndexToString, StringIndexer\nimport pyspark.sql\n\ndf = spark.createDataFrame(\n    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\"), (6, \"d\"), (7, \"d\"), (8, \"d\")],\n    [\"id\", \"category\"])\n\n#df.createOrReplaceTempView('ravi')\n#print spark.sql(\"select * from ravi where id = '2'\").show()\n#print ravi1.show()\nindexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nmodel = indexer.fit(df)\nindexed = model.transform(df)\n\nprint(\"Transformed string column '%s' to indexed column '%s'\"\n      % (indexer.getInputCol(), indexer.getOutputCol()))\nindexed.show()\n\nprint(\"StringIndexer will store labels in output column metadata\\n\")\n\nconverter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\nconverted = converter.transform(indexed)\n\nprint(\"Transformed indexed column '%s' back to original string column '%s' using \"\n      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\nconverted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+---+--------+\n| id|category|\n+---+--------+\n|  2|       c|\n+---+--------+\n\nNone\nTransformed string column &apos;category&apos; to indexed column &apos;categoryIndex&apos;\n+---+--------+-------------+\n| id|category|categoryIndex|\n+---+--------+-------------+\n|  0|       a|          0.0|\n|  1|       b|          3.0|\n|  2|       c|          2.0|\n|  3|       a|          0.0|\n|  4|       a|          0.0|\n|  5|       c|          2.0|\n|  6|       d|          1.0|\n|  7|       d|          1.0|\n|  8|       d|          1.0|\n+---+--------+-------------+\n\nStringIndexer will store labels in output column metadata\n\nTransformed indexed column &apos;categoryIndex&apos; back to original string column &apos;originalCategory&apos; using labels in metadata\n+---+-------------+----------------+\n| id|categoryIndex|originalCategory|\n+---+-------------+----------------+\n|  0|          0.0|               a|\n|  1|          3.0|               b|\n|  2|          2.0|               c|\n|  3|          0.0|               a|\n|  4|          0.0|               a|\n|  5|          2.0|               c|\n|  6|          1.0|               d|\n|  7|          1.0|               d|\n|  8|          1.0|               d|\n+---+-------------+----------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"category","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"indexed","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"category","nullable":true,"type":"string"},{"metadata":{"ml_attr":{"name":"categoryIndex","type":"nominal","vals":["a","d","c","b"]}},"name":"categoryIndex","nullable":false,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"converted","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"category","nullable":true,"type":"string"},{"metadata":{"ml_attr":{"name":"categoryIndex","type":"nominal","vals":["a","d","c","b"]}},"name":"categoryIndex","nullable":false,"type":"double"},{"metadata":{},"name":"originalCategory","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":"<span class=\"ansired\">AnalysisException</span>: u&quot;Table or view not found: &#96;global_temp&#96;.&#96;ravi&#96;; line 1 pos 14;\\n&apos;Project [*]\\n+- &apos;Filter (&apos;id = 2)\\n   +- &apos;UnresolvedRelation &#96;global_temp&#96;.&#96;ravi&#96;\\n&quot;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1859291325708966&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      7</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      8</span> df<span class=\"ansiyellow\">.</span>createOrReplaceTempView<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;ravi&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 9</span><span class=\"ansiyellow\"> </span><span class=\"ansigreen\">print</span> spark<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;select * from global_temp.ravi where id = &apos;2&apos;&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     10</span> <span class=\"ansired\">#print ravi1.show()</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     11</span> indexer <span class=\"ansiyellow\">=</span> StringIndexer<span class=\"ansiyellow\">(</span>inputCol<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;category&quot;</span><span class=\"ansiyellow\">,</span> outputCol<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;categoryIndex&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">sql</span><span class=\"ansiblue\">(self, sqlQuery)</span>\n<span class=\"ansigreen\">    554</span>         <span class=\"ansiyellow\">[</span>Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row1&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row2&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row3&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    555</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 556</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jsparkSession<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">(</span>sqlQuery<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_wrapped<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    557</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    558</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">2.0</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansigreen\">     68</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.AnalysisException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 69</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     70</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.catalyst.analysis&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     71</span>                 <span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AnalysisException</span>: u&quot;Table or view not found: &#96;global_temp&#96;.&#96;ravi&#96;; line 1 pos 14;\\n&apos;Project [*]\\n+- &apos;Filter (&apos;id = 2)\\n   +- &apos;UnresolvedRelation &#96;global_temp&#96;.&#96;ravi&#96;\\n&quot;</div>","workflows":[],"startTime":1512030595755,"submitTime":1512030595741,"finishTime":1512030597069,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e197c3a9-d197-4723-aca0-6253faa68933"},{"version":"CommandV1","origId":1859291325708967,"guid":"1a63863a-e325-459b-904a-b5e32f353876","subtype":"command","commandType":"auto","position":24.0,"command":"#OneHotEncoder maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which #expect continuous features, such as Logistic Regression, to use categorical features\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\n\ndf = spark.createDataFrame([\n    (0, \"a\"),\n    (1, \"b\"),\n    (2, \"c\"),\n    (3, \"a\"),\n    (4, \"a\"),\n    (5, \"c\"),\n    (6, \"d\")\n], [\"id\", \"category\"])\n\nstringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nmodel = stringIndexer.fit(df)\nindexed = model.transform(df)\n\nencoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\nencoded = encoder.transform(indexed)\nencoded.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+---+--------+-------------+-------------+\n| id|category|categoryIndex|  categoryVec|\n+---+--------+-------------+-------------+\n|  0|       a|          0.0|(3,[0],[1.0])|\n|  1|       b|          2.0|(3,[2],[1.0])|\n|  2|       c|          1.0|(3,[1],[1.0])|\n|  3|       a|          0.0|(3,[0],[1.0])|\n|  4|       a|          0.0|(3,[0],[1.0])|\n|  5|       c|          1.0|(3,[1],[1.0])|\n|  6|       d|          3.0|    (3,[],[])|\n+---+--------+-------------+-------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"category","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"indexed","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"category","nullable":true,"type":"string"},{"metadata":{"ml_attr":{"name":"categoryIndex","type":"nominal","vals":["a","c","b","d"]}},"name":"categoryIndex","nullable":false,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"encoded","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"category","nullable":true,"type":"string"},{"metadata":{"ml_attr":{"name":"categoryIndex","type":"nominal","vals":["a","c","b","d"]}},"name":"categoryIndex","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"binary":[{"idx":0,"name":"a"},{"idx":1,"name":"c"},{"idx":2,"name":"b"}]},"num_attrs":3}},"name":"categoryVec","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1511940518380,"submitTime":1511940518366,"finishTime":1511940519070,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2852207c-5ad7-41fa-9fe1-05b031606566"},{"version":"CommandV1","origId":1859291325708968,"guid":"dd4e3156-218d-4c03-b09e-69d4a599c2bc","subtype":"command","commandType":"auto","position":25.0,"command":"#Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which specifies #the p-norm used for normalization. (p=2p=2 by default.)\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.linalg import Vectors\n\ndataFrame = spark.createDataFrame([\n    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n], [\"id\", \"features\"])\n\n# Normalize each Vector using $L^1$ norm.\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\nl1NormData = normalizer.transform(dataFrame)\nprint(\"Normalized using L^1 norm\")\nl1NormData.show()\n\n# Normalize each Vector using $L^\\infty$ norm.\nlInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\nprint(\"Normalized using L^inf norm\")\nlInfNormData.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Normalized using L^1 norm\n+---+--------------+------------------+\n| id|      features|      normFeatures|\n+---+--------------+------------------+\n|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n+---+--------------+------------------+\n\nNormalized using L^inf norm\n+---+--------------+--------------+\n| id|      features|  normFeatures|\n+---+--------------+--------------+\n|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n+---+--------------+--------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"dataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null},{"name":"l1NormData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"normFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null},{"name":"lInfNormData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"normFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":"<span class=\"ansired\">AnalysisException</span>: u&apos;Path does not exist: dbfs:/data/mllib/sample_libsvm_data.txt;&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1859291325708968&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">from</span> pyspark<span class=\"ansiyellow\">.</span>ml<span class=\"ansiyellow\">.</span>feature <span class=\"ansigreen\">import</span> VectorIndexer<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 3</span><span class=\"ansiyellow\"> </span>data <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>format<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;libsvm&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>load<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;data/mllib/sample_libsvm_data.txt&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> indexer <span class=\"ansiyellow\">=</span> VectorIndexer<span class=\"ansiyellow\">(</span>inputCol<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;features&quot;</span><span class=\"ansiyellow\">,</span> outputCol<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;indexed&quot;</span><span class=\"ansiyellow\">,</span> maxCategories<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">10</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansicyan\">load</span><span class=\"ansiblue\">(self, path, format, schema, **options)</span>\n<span class=\"ansigreen\">    157</span>         self<span class=\"ansiyellow\">.</span>options<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">**</span>options<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    158</span>         <span class=\"ansigreen\">if</span> isinstance<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">,</span> basestring<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 159</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_df<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jreader<span class=\"ansiyellow\">.</span>load<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    160</span>         <span class=\"ansigreen\">elif</span> path <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    161</span>             <span class=\"ansigreen\">if</span> type<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">!=</span> list<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansigreen\">     68</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.AnalysisException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 69</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     70</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.catalyst.analysis&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     71</span>                 <span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AnalysisException</span>: u&apos;Path does not exist: dbfs:/data/mllib/sample_libsvm_data.txt;&apos;</div>","workflows":[],"startTime":1512030926523,"submitTime":1512030926498,"finishTime":1512030927160,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b0905b1d-2032-4b4e-a551-7c574b6ccd8b"},{"version":"CommandV1","origId":1150384569023011,"guid":"51cfcd97-a398-494e-a75e-f85564e83212","subtype":"command","commandType":"auto","position":26.0,"command":"from pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.linalg import Vectors\n\ndataFrame = spark.createDataFrame([\n    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n], [\"id\", \"features\"])\n\nscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n\n# Compute summary statistics and generate MinMaxScalerModel\nscalerModel = scaler.fit(dataFrame)\n\n# rescale each feature to range [min, max].\nscaledData = scalerModel.transform(dataFrame)\nprint(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\nscaledData.select(\"features\", \"scaledFeatures\").show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Features scaled to range: [0.000000, 1.000000]\n+--------------+--------------+\n|      features|scaledFeatures|\n+--------------+--------------+\n|[1.0,0.1,-1.0]| [0.0,0.0,0.0]|\n| [2.0,1.1,1.0]| [0.5,0.1,0.5]|\n|[3.0,10.1,3.0]| [1.0,1.0,1.0]|\n+--------------+--------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"dataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null},{"name":"scaledData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"scaledFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1512031286759,"submitTime":1512031286728,"finishTime":1512031289128,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b29f636b-3c06-4449-84eb-5161bdd747d7"},{"version":"CommandV1","origId":1150384569023012,"guid":"a4746f71-8650-4d76-b6a6-ad33ad967017","subtype":"command","commandType":"auto","position":27.0,"command":"from pyspark.ml.feature import Bucketizer\n\nsplits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n\ndata = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,), (201.2,), (21.2,)]\ndataFrame = spark.createDataFrame(data, [\"features\"])\n\nbucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n\n# Transform original data into its bucket index.\nbucketedData = bucketizer.transform(dataFrame)\n\nprint(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\nbucketedData.show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Bucketizer output with 4 buckets\n+--------+----------------+\n|features|bucketedFeatures|\n+--------+----------------+\n|  -999.9|             0.0|\n|    -0.5|             1.0|\n|    -0.3|             1.0|\n|     0.0|             2.0|\n|     0.2|             2.0|\n|   999.9|             3.0|\n|   201.2|             3.0|\n|    21.2|             3.0|\n+--------+----------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"dataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"features","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"bucketedData","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"features","nullable":true,"type":"double"},{"metadata":{"ml_attr":{"ord":true,"type":"nominal","vals":["-Infinity, -0.5","-0.5, 0.0","0.0, 0.5","0.5, Infinity"]}},"name":"bucketedFeatures","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":"<span class=\"ansired\">IllegalArgumentException</span>: u&apos;Bucketizer_40838d583daae166d689 parameter splits given invalid value [-Infinity,-0.5,0.0,0.5,0.5,0.2,Infinity].&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1150384569023012&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      9</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     10</span> <span class=\"ansired\"># Transform original data into its bucket index.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 11</span><span class=\"ansiyellow\"> </span>bucketedData <span class=\"ansiyellow\">=</span> bucketizer<span class=\"ansiyellow\">.</span>transform<span class=\"ansiyellow\">(</span>dataFrame<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     12</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     13</span> <span class=\"ansigreen\">print</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Bucketizer output with %d buckets&quot;</span> <span class=\"ansiyellow\">%</span> <span class=\"ansiyellow\">(</span>len<span class=\"ansiyellow\">(</span>bucketizer<span class=\"ansiyellow\">.</span>getSplits<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">-</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansicyan\">transform</span><span class=\"ansiblue\">(self, dataset, params)</span>\n<span class=\"ansigreen\">    103</span>                 <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>copy<span class=\"ansiyellow\">(</span>params<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>_transform<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    104</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 105</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_transform<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    106</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    107</span>             <span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Params must be a param map but got %s.&quot;</span> <span class=\"ansiyellow\">%</span> type<span class=\"ansiyellow\">(</span>params<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansicyan\">_transform</span><span class=\"ansiblue\">(self, dataset)</span>\n<span class=\"ansigreen\">    278</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    279</span>     <span class=\"ansigreen\">def</span> _transform<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 280</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_transfer_params_to_java<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    281</span>         <span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_java_obj<span class=\"ansiyellow\">.</span>transform<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> dataset<span class=\"ansiyellow\">.</span>sql_ctx<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    282</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansicyan\">_transfer_params_to_java</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    122</span>         <span class=\"ansigreen\">for</span> param <span class=\"ansigreen\">in</span> self<span class=\"ansiyellow\">.</span>params<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    123</span>             <span class=\"ansigreen\">if</span> param <span class=\"ansigreen\">in</span> paramMap<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 124</span><span class=\"ansiyellow\">                 </span>pair <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_make_java_param_pair<span class=\"ansiyellow\">(</span>param<span class=\"ansiyellow\">,</span> paramMap<span class=\"ansiyellow\">[</span>param<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    125</span>                 self<span class=\"ansiyellow\">.</span>_java_obj<span class=\"ansiyellow\">.</span>set<span class=\"ansiyellow\">(</span>pair<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    126</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansicyan\">_make_java_param_pair</span><span class=\"ansiblue\">(self, param, value)</span>\n<span class=\"ansigreen\">    113</span>         java_param <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_java_obj<span class=\"ansiyellow\">.</span>getParam<span class=\"ansiyellow\">(</span>param<span class=\"ansiyellow\">.</span>name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    114</span>         java_value <span class=\"ansiyellow\">=</span> _py2java<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> value<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 115</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> java_param<span class=\"ansiyellow\">.</span>w<span class=\"ansiyellow\">(</span>java_value<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    116</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    117</span>     <span class=\"ansigreen\">def</span> _transfer_params_to_java<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     77</span>                 <span class=\"ansigreen\">raise</span> QueryExecutionException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     78</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;java.lang.IllegalArgumentException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 79</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> IllegalArgumentException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     80</span>             <span class=\"ansigreen\">raise</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     81</span>     <span class=\"ansigreen\">return</span> deco<span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">IllegalArgumentException</span>: u&apos;Bucketizer_40838d583daae166d689 parameter splits given invalid value [-Infinity,-0.5,0.0,0.5,0.5,0.2,Infinity].&apos;</div>","workflows":[],"startTime":1512035381031,"submitTime":1512035381010,"finishTime":1512035381636,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"54f53fca-64e7-4ca9-9477-e735511847a5"},{"version":"CommandV1","origId":1150384569023013,"guid":"7c66768e-d1be-46c2-b96d-34b222a705d1","subtype":"command","commandType":"auto","position":28.0,"command":"from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ndataset = spark.createDataFrame(\n    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n\nassembler = VectorAssembler(\n    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n    outputCol=\"features\")\n\noutput = assembler.transform(dataset)\nprint(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\noutput.select(\"features\", \"clicked\").show(truncate=False)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Assembled columns &apos;hour&apos;, &apos;mobile&apos;, &apos;userFeatures&apos; to vector column &apos;features&apos;\n+-----------------------+-------+\n|features               |clicked|\n+-----------------------+-------+\n|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n+-----------------------+-------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"dataset","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"hour","nullable":true,"type":"long"},{"metadata":{},"name":"mobile","nullable":true,"type":"double"},{"metadata":{},"name":"userFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"clicked","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"output","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"hour","nullable":true,"type":"long"},{"metadata":{},"name":"mobile","nullable":true,"type":"double"},{"metadata":{},"name":"userFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"clicked","nullable":true,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"hour"},{"idx":1,"name":"mobile"},{"idx":2,"name":"userFeatures_0"},{"idx":3,"name":"userFeatures_1"},{"idx":4,"name":"userFeatures_2"}]},"num_attrs":5}},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1512035532453,"submitTime":1512035532431,"finishTime":1512035533189,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"12a94595-5e08-4a31-a99b-c0eac66cd8c0"},{"version":"CommandV1","origId":1150384569023014,"guid":"f2ab1e79-b203-402b-a2aa-0c23eff2bb94","subtype":"command","commandType":"auto","position":29.0,"command":"#The Imputer transformer completes missing values in a dataset, either using the mean or the median of the columns in which the missing values are #located. The input columns should be of DoubleType or FloatType. Currently Imputer does not support categorical features and possibly creates incorrect #values for columns containing categorical features.\nfrom pyspark.ml.feature import Imputer\n\ndf = spark.createDataFrame([\n    (1.0, float(\"nan\")),\n    (2.0, float(\"nan\")),\n    (float(\"nan\"), 3.0),\n    (4.0, 4.0),\n    (5.0, 5.0),\n    (6.0, float(\"nan\")),\n    (float(\"nan\"), 4.0)\n], [\"a\", \"b\"])\n\nimputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\nmodel = imputer.fit(df)\n\nmodel.transform(df).show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+---+---+-----+-----+\n|  a|  b|out_a|out_b|\n+---+---+-----+-----+\n|1.0|NaN|  1.0|  4.0|\n|2.0|NaN|  2.0|  4.0|\n|NaN|3.0|  3.6|  3.0|\n|4.0|4.0|  4.0|  4.0|\n|5.0|5.0|  5.0|  5.0|\n|6.0|NaN|  6.0|  4.0|\n|NaN|4.0|  3.6|  4.0|\n+---+---+-----+-----+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"a","nullable":true,"type":"double"},{"metadata":{},"name":"b","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":null,"error":null,"workflows":[],"startTime":1512037840754,"submitTime":1512037840737,"finishTime":1512037841969,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c7372103-ebad-4ca5-afc3-8473be7e2a54"},{"version":"CommandV1","origId":1150384569023015,"guid":"b163c7a2-aef5-4cd3-abb5-51bf5875df9b","subtype":"command","commandType":"auto","position":30.0,"command":"from pyspark.ml.feature import RFormula\n\ndataset = spark.createDataFrame(\n    [(7, \"US\", 18, 1.0),\n     (8, \"CA\", 12, 0.0),\n     (9, \"NZ\", 15, 0.0),\n     (6, \"AU\", 16, 1.0)],\n    [\"id\", \"country\", \"hour\", \"clicked\"])\n\nformula = RFormula(\n    formula=\"clicked ~ country + hour\",\n    featuresCol=\"features\",\n    labelCol=\"label\")\n\noutput = formula.fit(dataset).transform(dataset)\noutput.select(\"features\", \"label\").show()\n\nformula1 = RFormula(\n    formula=\"clicked ~ country : hour\",\n    featuresCol=\"features\",\n    labelCol=\"label\")\n\noutput = formula1.fit(dataset).transform(dataset)\noutput.select(\"features\", \"label\").show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+------------------+-----+\n|          features|label|\n+------------------+-----+\n|    (4,[3],[18.0])|  1.0|\n|[0.0,0.0,1.0,12.0]|  0.0|\n|[0.0,1.0,0.0,15.0]|  0.0|\n|[1.0,0.0,0.0,16.0]|  1.0|\n+------------------+-----+\n\n+--------------+-----+\n|      features|label|\n+--------------+-----+\n|(4,[3],[18.0])|  1.0|\n|(4,[2],[12.0])|  0.0|\n|(4,[1],[15.0])|  0.0|\n|(4,[0],[16.0])|  1.0|\n+--------------+-----+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"dataset","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"country","nullable":true,"type":"string"},{"metadata":{},"name":"hour","nullable":true,"type":"long"},{"metadata":{},"name":"clicked","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"output","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"long"},{"metadata":{},"name":"country","nullable":true,"type":"string"},{"metadata":{},"name":"hour","nullable":true,"type":"long"},{"metadata":{},"name":"clicked","nullable":true,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"country_AU:hour"},{"idx":1,"name":"country_NZ:hour"},{"idx":2,"name":"country_CA:hour"},{"idx":3,"name":"country_US:hour"}]},"num_attrs":4}},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"label","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}]},"errorSummary":"<span class=\"ansired\">IllegalArgumentException</span>: u&apos;Could not parse formula: clicked - country : hour&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1150384569023015&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     20</span>     labelCol=&quot;label&quot;)\n<span class=\"ansigreen\">     21</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 22</span><span class=\"ansiyellow\"> </span>output <span class=\"ansiyellow\">=</span> formula1<span class=\"ansiyellow\">.</span>fit<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>transform<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     23</span> output<span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;features&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;label&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansicyan\">fit</span><span class=\"ansiblue\">(self, dataset, params)</span>\n<span class=\"ansigreen\">     62</span>                 <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>copy<span class=\"ansiyellow\">(</span>params<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>_fit<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     63</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 64</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_fit<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     66</span>             raise ValueError(&quot;Params must be either a param map or a list/tuple of param maps, &quot;\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansicyan\">_fit</span><span class=\"ansiblue\">(self, dataset)</span>\n<span class=\"ansigreen\">    263</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    264</span>     <span class=\"ansigreen\">def</span> _fit<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 265</span><span class=\"ansiyellow\">         </span>java_model <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_fit_java<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    266</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_create_model<span class=\"ansiyellow\">(</span>java_model<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    267</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansicyan\">_fit_java</span><span class=\"ansiblue\">(self, dataset)</span>\n<span class=\"ansigreen\">    260</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    261</span>         self<span class=\"ansiyellow\">.</span>_transfer_params_to_java<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 262</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_java_obj<span class=\"ansiyellow\">.</span>fit<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    263</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    264</span>     <span class=\"ansigreen\">def</span> _fit<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     77</span>                 <span class=\"ansigreen\">raise</span> QueryExecutionException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     78</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;java.lang.IllegalArgumentException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 79</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> IllegalArgumentException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     80</span>             <span class=\"ansigreen\">raise</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     81</span>     <span class=\"ansigreen\">return</span> deco<span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">IllegalArgumentException</span>: u&apos;Could not parse formula: clicked - country : hour&apos;</div>","workflows":[],"startTime":1512127078948,"submitTime":1512127040362,"finishTime":1512127084237,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d409606a-c508-40ef-be68-ff61ab2efd18"}],"dashboards":[],"guid":"0cba60f4-8e6d-4a79-9da1-271b78c9eb9a","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/bc1c85971ad864da65b19d4f80abe81e111e7bfd8db9cac8f69dc1d94b874689/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":0,"guid":"43693108-d9c1-4632-a68c-aa734618921a","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{}};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/bc1c85971ad864da65b19d4f80abe81e111e7bfd8db9cac8f69dc1d94b874689/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>