{"cells":[{"cell_type":"code","source":["#data in textfile separated by ::\n#so loading as textFile\nlines = spark.read.text('/FileStore/tables/sample_movielens_ratings.txt').rdd"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["print lines.take(5)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#Split based on ::\ndata = lines.map(lambda row:row.value.split('::'))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql import Row\nrdd = data.map(lambda d: Row(userId= int(d[0]), \n                       movieId=int(d[1]), \n                       rating=int(d[2]), \n                       timestamp=long(d[3]) ))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df = spark.createDataFrame(rdd)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df.show(10)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%fs ls /databricks-datasets/structured-streaming/events/"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ninputPath = \"/databricks-datasets/structured-streaming/events/\"\n\n# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\njsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n\n# Static DataFrame representing data in the JSON files\nstaticInputDF = (\n  spark\n    .read\n    .schema(jsonSchema)\n    .json(inputPath)\n)\n\ndisplay(staticInputDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import *      # for window() function\n\nstaticCountsDF = (\n  staticInputDF\n    .groupBy(\n       staticInputDF.action, \n       window(staticInputDF.time, \"1 hour\"))    \n    .count()\n)\nstaticCountsDF.cache()\n\n# Register the DataFrame as table 'static_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#%sql select action, sum(count) as total_count from static_counts group by action"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time,count from static_counts order by action, time"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(jsonSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .json(inputPath)\n)\n\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.action, \n      window(streamingInputDF.time, \"1 hour\"))\n    .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"Recomendation","notebookId":3687057758492121},"nbformat":4,"nbformat_minor":0}
