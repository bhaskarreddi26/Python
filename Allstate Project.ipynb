{"cells":[{"cell_type":"code","source":["data_test = spark.read.csv('/FileStore/tables/test.csv',header=True, inferSchema=True)\ndata_train = spark.read.csv('/FileStore/tables/train.csv', header=True, inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["print data_test.count()\nprint data_train.count()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["data_train = data_train.withColumnRenamed(\"loss\",\"lable\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["[trainingdata, validationdata] = data_train.randomSplit([0.7, 0.3])"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["trainingdata.cache()\nvalidationdata.cache()\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["columns = data_train.columns"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["cat_cols = list(filter(lambda s: s.startswith(\"cat\"), columns))\ncont_cols = list(filter(lambda s: s.startswith(\"cont\"), columns))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["startIndexList = []\nimport pyspark.ml.feature as ft"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["for cols in cat_cols:\n  startIndexList.append(ft.StringIndexer(inputCol=cols, outputCol=\"idx\"+cols[3:]))\n\nidx_cat_cols = list(map(lambda s: s.replace(\"cat\", 'idx'), cat_cols))\n  "],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml import Pipeline, PipelineModel\nassembler = ft.VectorAssembler(inputCols = idx_cat_cols+cont_cols, outputCol='features')"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\nestimator = RandomForestRegressor(numTrees=3,maxDepth=5, maxBins=1100, featuresCol=\"features\",labelCol=\"label\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["stages = []\nstages.extend(startIndexList)\nstages.append(assembler)\nstages.append(estimator) \n\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["pipeline = Pipeline(stages=stages)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["model = pipeline.fit(trainingdata)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["model.transform()"],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"Allstate Project","notebookId":690813988709975},"nbformat":4,"nbformat_minor":0}
