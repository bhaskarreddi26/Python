{"cells":[{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nimport pyspark.ml.evaluation as ev\n# Load training data\ntraining = spark.read.format(\"libsvm\").load(\"/FileStore/tables/sample_libsvm_data.txt\")\n\n#print training.show()\n\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)\n\n(trainingData, testData) = training.randomSplit([0.7, 0.3])\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, labelCol='label')\n\npipeline = Pipeline(stages=[\n        labelIndexer, \n        featureIndexer, \n        lr\n    ])\n\n# Fit the model\nlrModel = lr.fit(trainingData)\ntest_model = lrModel.transform(testData)\n\n#print ('display LR model', test_model.take(1))\nevaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='label')\n\nprint(' BinaryClassification areaUnderROC:', evaluator.evaluate(test_model, \n     {evaluator.metricName: 'areaUnderROC'}))\nprint('BinaryClassification areaUnderPR:', evaluator.evaluate(test_model, {evaluator.metricName: 'areaUnderPR'}))\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import pyspark.ml.tuning as tune\nlogistic = LogisticRegression(\n    labelCol='label')\n\ngrid = tune.ParamGridBuilder() \\\n    .addGrid(logistic.maxIter,  \n             [0, 1, 2, 10, 50]) \\\n    .addGrid(logistic.regParam, \n             [0.002, 0.003, 0.001, 0.005, 0.3]) \\\n    .build()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["cv = tune.CrossValidator(\n    estimator=logistic, \n    estimatorParamMaps=grid, \n    evaluator=evaluator\n)\npipeline = Pipeline(stages=[featureIndexer])\ndata_transformer = pipeline.fit(trainingData)\ncvModel = cv.fit(data_transformer.transform(trainingData))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["results = [\n    (\n        [\n            {key.name: paramValue} \n            for key, paramValue \n            in zip(\n                params.keys(), \n                params.values())\n        ], metric\n    ) \n    for params, metric \n    in zip(\n        cvModel.getEstimatorParamMaps(), \n        cvModel.avgMetrics\n    )\n]\n\nsorted(results, \n       key=lambda el: el[1], \n       reverse=True)[0]"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#The spark.ml implementation supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical #features.\n#Param name\tType(s)\tDefault\tDescription\n#labelCol\tDouble\t\"label\"\tLabel to predict\n#featuresCol\tVector\t\"features\"\tFeature vector\n\n#GBTs train one tree at a time, so they can take longer to train than random forests. Random Forests can train multiple trees in parallel.\n#On the other hand, it is often reasonable to use smaller (shallower) trees with GBTs than with Random Forests, and training smaller trees takes less #time.\n#Random Forests can be less prone to overfitting. Training more trees in a Random Forest reduces the likelihood of overfitting, but training more trees #with GBTs increases the likelihood of overfitting. (In statistical language, Random Forests reduce variance by using more trees, whereas GBTs reduce #bias by using more trees.)\n#Random Forests can be easier to tune since performance improves monotonically with the number of trees (whereas performance can start to decrease for #GBTs if the number of trees grows too large).\n\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nimport pyspark.ml.evaluation as cv\n\n# Load the data stored in LIBSVM format as a DataFrame.\ndata = spark.read.format(\"libsvm\").load(\"/FileStore/tables/sample_libsvm_data.txt\")\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = ev.MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint (\"First 5:\", accuracy)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml import Pipeline"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nimport pyspark.ml.evaluation as ev\n\ndata = spark.read.format(\"libsvm\").load(\"/FileStore/tables/sample_libsvm_data.txt\")\n\n\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\ngbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\nmodel = pipeline.fit(trainingData)\npredictions = model.transform(testData)\nevaluator = ev.MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\ngbtModel = model.stages[2]\nprint(gbtModel)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"ml-classification-regression","notebookId":3535249741746098},"nbformat":4,"nbformat_minor":0}
