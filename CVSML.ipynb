{"cells":[{"cell_type":"code","source":["DF = spark.read.csv('/FileStore/tables/UsersImportMinimumSample.csv', header=True, inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["DF.printSchema()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["DF.show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["import pyspark.ml.feature as ft\ntransfer_srm = ft.StringIndexer(inputCol='SRM_SaaS_ES', outputCol='SRM_SaaS_ES_en')\ntransfer_mxp = ft.StringIndexer(inputCol='MXPERUSERInterface', outputCol='MXPERUSERInterface_en')\ntransfer_Add = ft.StringIndexer(inputCol='Add', outputCol='Add_en')\ntransfer_EN = ft.StringIndexer(inputCol='EN', outputCol='EN_en')"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.stat import Correlation\n\ndata = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\ndf = spark.createDataFrame(data, [\"features\"])\n\nr1 = Correlation.corr(df, \"features\").head()\nprint(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n\nr2 = Correlation.corr(df, \"features\", \"spearman\").head()\nprint(\"Spearman correlation matrix:\\n\" + str(r2[0]))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.stat import ChiSquareTest\n\ndata = [(0.0, Vectors.dense(0.5, 10.0)),\n        (0.0, Vectors.dense(1.5, 20.0)),\n        (1.0, Vectors.dense(1.5, 30.0)),\n        (0.0, Vectors.dense(3.5, 30.0)),\n        (0.0, Vectors.dense(3.5, 40.0)),\n        (1.0, Vectors.dense(3.5, 40.0))]\ndf = spark.createDataFrame(data, [\"label\", \"features\"])\n\nr = ChiSquareTest.test(df, \"features\", \"label\").head()\nprint(\"pValues: \" + str(r.pValues))\nprint(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\nprint(\"statistics: \" + str(r.statistics))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentenceData = spark.createDataFrame([\n    (0.0, \"Hi I heard about Spark\"),\n    (0.0, \"I wish Java could use case classes\"),\n    (1.0, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData)\n\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(wordsData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)\n\nrescaledData.select(\"label\", \"features\").show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentenceData = spark.createDataFrame([\n    (0.0, \"Hi I heard about Spark\"),\n    (0.0, \"I wish Java could use case classes\"),\n    (1.0, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["wordsData.show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\nfeaturizedData = hashingTF.transform(wordsData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["featurizedData.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["rescaledData.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#Word2Vec is an Estimator which takes sequences of words representing documents and trains a Word2VecModel. The model maps each word to a unique\n#fixed-size vector.\nfrom pyspark.ml.feature import Word2Vec\n\n# Input data: Each row is a bag of words from a sentence or document.\ndocumentDF = spark.createDataFrame([\n    (\"Hi I heard about Spark\".split(\" \"), ),\n    (\"I wish Java could use case classes\".split(\" \"), ),\n    (\"Logistic regression models are neat\".split(\" \"), )\n], [\"text\"])\n\n# Learn a mapping from words to Vectors.\nword2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\nmodel = word2Vec.fit(documentDF)\n\nresult = model.transform(documentDF)\nfor row in result.collect():\n    text, vector = row\n    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. \nfrom pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a d e\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=4, minDF=2.0)\n\nmodel = cv.fit(df)\n\nresult = model.transform(df)\nresult.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#Feature Transformers\n#Tokenizer take a input as a sentence and output as a breaking it into individual words\n#RegexTokenizer is taking input as a sentence and output as a words. It's more advanced  tokenization based on regular expression (regex) matching. By #default, the parameter “pattern” (regex, default: \"\\\\s+\") is used as delimiters to split the input text. \nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType\n\nsentenceDataFrame = spark.createDataFrame([\n    (0, \"Hi I heard about Spark\"),\n    (1, \"I wish Java could use case classes\"),\n    (2, \"Logistic,regression,models,are,neat,reddy\")\n], [\"id\", \"sentence\"])\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n# alternatively, pattern=\"\\\\w+\", gaps(False)\n\ncountTokens = udf(lambda words: len(words), IntegerType())\n\ntokenized = tokenizer.transform(sentenceDataFrame)\ntokenized.select(\"sentence\", \"words\")\\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\nregexTokenized = regexTokenizer.transform(sentenceDataFrame)\nregexTokenized.select(\"sentence\", \"words\") \\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# StopWordRemover is remove the single letter or unnecessary words from the sentence\nfrom pyspark.ml.feature import StopWordsRemover\n\nsentenceData = spark.createDataFrame([\n    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n], [\"id\", \"raw\"])\n\nremover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\nremover.transform(sentenceData).show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#NGram taking the words as input and create the n sequence of words\nfrom pyspark.ml.feature import NGram\n\nwordDataFrame = spark.createDataFrame([\n    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n], [\"id\", \"words\"])\n\nngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n\nngramDataFrame = ngram.transform(wordDataFrame)\nngramDataFrame.select(\"ngrams\").show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#Binarization is the process of thresholding numerical features to binary (0/1) features.\nfrom pyspark.ml.feature import Binarizer\n\ncontinuousDataFrame = spark.createDataFrame([\n    (0, 0.1),\n    (1, 0.8),\n    (2, 0.2),\n    (3, 1.2)\n], [\"id\", \"feature\"])\n\nbinarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n\nbinarizedDataFrame = binarizer.transform(continuousDataFrame)\n\nprint(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\nbinarizedDataFrame.show()\nbinarizer.write().save('/FileStore/tables/reddyb.txt')"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["#PCA trains a model to project vectors to a lower dimensional space of the top k principal components.\nfrom pyspark.ml.feature import PCA\nfrom pyspark.ml.linalg import Vectors\n\ndata = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\ndf = spark.createDataFrame(data, [\"features\"])\nprint df.show()\npca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\nmodel = pca.fit(df)\n\nresult = model.transform(df).select(\"pcaFeatures\")\nresult.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#PolynomialExpansionis the process of expanding your features into a polynomial space, which is formulated by an n-degree combination of original #dimensions\nfrom pyspark.ml.feature import PolynomialExpansion\nfrom pyspark.ml.linalg import Vectors\n\ndf = spark.createDataFrame([\n    (Vectors.dense([2.0, 1.0]),),\n    (Vectors.dense([0.0, 0.0]),),\n    (Vectors.dense([3.0, -1.0]),)\n], [\"features\"])\n\npolyExpansion = PolynomialExpansion(degree=2, inputCol=\"features\", outputCol=\"polyFeatures\")\npolyDF = polyExpansion.transform(df)\n\npolyDF.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\ndf = spark.createDataFrame(\n    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\"), (6, \"d\")],\n    [\"id\", \"category\"])\n\nindexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n\nindexed = indexer.fit(df).transform(df)\nindexed.show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from pyspark.ml.feature import IndexToString, StringIndexer\nimport pyspark.sql\n\ndf = spark.createDataFrame(\n    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\"), (6, \"d\"), (7, \"d\"), (8, \"d\")],\n    [\"id\", \"category\"])\n\n#df.createOrReplaceTempView('ravi')\n#print spark.sql(\"select * from ravi where id = '2'\").show()\n#print ravi1.show()\nindexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nmodel = indexer.fit(df)\nindexed = model.transform(df)\n\nprint(\"Transformed string column '%s' to indexed column '%s'\"\n      % (indexer.getInputCol(), indexer.getOutputCol()))\nindexed.show()\n\nprint(\"StringIndexer will store labels in output column metadata\\n\")\n\nconverter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\nconverted = converter.transform(indexed)\n\nprint(\"Transformed indexed column '%s' back to original string column '%s' using \"\n      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\nconverted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#OneHotEncoder maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which #expect continuous features, such as Logistic Regression, to use categorical features\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\n\ndf = spark.createDataFrame([\n    (0, \"a\"),\n    (1, \"b\"),\n    (2, \"c\"),\n    (3, \"a\"),\n    (4, \"a\"),\n    (5, \"c\"),\n    (6, \"d\")\n], [\"id\", \"category\"])\n\nstringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nmodel = stringIndexer.fit(df)\nindexed = model.transform(df)\n\nencoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\nencoded = encoder.transform(indexed)\nencoded.show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which specifies #the p-norm used for normalization. (p=2p=2 by default.)\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.linalg import Vectors\n\ndataFrame = spark.createDataFrame([\n    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n], [\"id\", \"features\"])\n\n# Normalize each Vector using $L^1$ norm.\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\nl1NormData = normalizer.transform(dataFrame)\nprint(\"Normalized using L^1 norm\")\nl1NormData.show()\n\n# Normalize each Vector using $L^\\infty$ norm.\nlInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\nprint(\"Normalized using L^inf norm\")\nlInfNormData.show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.linalg import Vectors\n\ndataFrame = spark.createDataFrame([\n    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n], [\"id\", \"features\"])\n\nscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n\n# Compute summary statistics and generate MinMaxScalerModel\nscalerModel = scaler.fit(dataFrame)\n\n# rescale each feature to range [min, max].\nscaledData = scalerModel.transform(dataFrame)\nprint(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\nscaledData.select(\"features\", \"scaledFeatures\").show()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.ml.feature import Bucketizer\n\nsplits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n\ndata = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,), (201.2,), (21.2,)]\ndataFrame = spark.createDataFrame(data, [\"features\"])\n\nbucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n\n# Transform original data into its bucket index.\nbucketedData = bucketizer.transform(dataFrame)\n\nprint(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\nbucketedData.show()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ndataset = spark.createDataFrame(\n    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n\nassembler = VectorAssembler(\n    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n    outputCol=\"features\")\n\noutput = assembler.transform(dataset)\nprint(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\noutput.select(\"features\", \"clicked\").show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["#The Imputer transformer completes missing values in a dataset, either using the mean or the median of the columns in which the missing values are #located. The input columns should be of DoubleType or FloatType. Currently Imputer does not support categorical features and possibly creates incorrect #values for columns containing categorical features.\nfrom pyspark.ml.feature import Imputer\n\ndf = spark.createDataFrame([\n    (1.0, float(\"nan\")),\n    (2.0, float(\"nan\")),\n    (float(\"nan\"), 3.0),\n    (4.0, 4.0),\n    (5.0, 5.0),\n    (6.0, float(\"nan\")),\n    (float(\"nan\"), 4.0)\n], [\"a\", \"b\"])\n\nimputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\nmodel = imputer.fit(df)\n\nmodel.transform(df).show()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\n\ndataset = spark.createDataFrame(\n    [(7, \"US\", 18, 1.0),\n     (8, \"CA\", 12, 0.0),\n     (9, \"NZ\", 15, 0.0),\n     (6, \"AU\", 16, 1.0)],\n    [\"id\", \"country\", \"hour\", \"clicked\"])\n\nformula = RFormula(\n    formula=\"clicked ~ country + hour\",\n    featuresCol=\"features\",\n    labelCol=\"label\")\n\noutput = formula.fit(dataset).transform(dataset)\noutput.select(\"features\", \"label\").show()\n\nformula1 = RFormula(\n    formula=\"clicked ~ country : hour\",\n    featuresCol=\"features\",\n    labelCol=\"label\")\n\noutput = formula1.fit(dataset).transform(dataset)\noutput.select(\"features\", \"label\").show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31}],"metadata":{"name":"CVSML","notebookId":1564452080561839},"nbformat":4,"nbformat_minor":0}
