{"cells":[{"cell_type":"code","source":["sorted(sc.parallelize([3, 1, 2, 2, 1, 2, 2], 2).countByValue().items())"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["rdd = sc.parallelize([(3, 1) , (2, 2), (1, 2), (2, 3), (2, 3)])\nprint sorted(rdd.countByKey().items())\nprint sorted(rdd.countByValue().items())"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from operator import mul\nsc.parallelize([1, 2, 3, 4, 5]).fold(1, mul)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["rdd = sc.parallelize([1, 2, 3, 4, 5, 6], 3)\nprint rdd.getNumPartitions()\nprint rdd.getStorageLevel()\nprint rdd.glom().collect()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df = spark.read.csv('/FileStore/tables/C2ImportFamRelSample.csv', header=True, inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df.show(10)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pysprak import SparkContext\nspark = SparkContext.builder.nameApp(\"apc\").getOrCreate()\ndf = spark.read.csv('/FileStore/tables/ParentTeacherConference.csv', header=True, inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df.cov('Category', 'Maximum Seats')"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df.describe()['Category', 'Maximum Seats'].show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df.filter(\"Category==2\")['Start Time', 'End Time', 'Location'].count()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["import pyspark.ml.feature as ft\n\nstart_dt = ft.StringIndexer(inputCol='Start Date', outputCol='start_dt')\nstart_tm = ft.StringIndexer(inputCol='Start Time', outputCol='start_tm')\nend_dt = ft.StringIndexer(inputCol='End Date', outputCol='end_dt')\nend_tm = ft.StringIndexer(inputCol='End Time', outputCol='end_tm')\nevent_tt = ft.StringIndexer(inputCol='Event Title', outputCol='event_tt')\nevent_ds = ft.StringIndexer(inputCol='Event Description', outputCol='event_ds')\nevent_ad = ft.StringIndexer(inputCol='All Day Event', outputCol='event_ad')\nend_nt = ft.StringIndexer(inputCol='No End Time', outputCol='end_nt')\nContact_ad = ft.StringIndexer(inputCol='Contact', outputCol='Contact_ad')\nContact_em = ft.StringIndexer(inputCol='Contact Email', outputCol='Contact_em')\nContact_ph = ft.StringIndexer(inputCol='Contact Phone', outputCol='Contact_ph')\nLocation_ad = ft.StringIndexer(inputCol='Location', outputCol='Location_ad')\nRegistration_ad = ft.StringIndexer(inputCol='Registration', outputCol='Registration_ad')\nlast_dr = ft.StringIndexer(inputCol='Last Date To Register', outputCol='last_dr')\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["featurec = ft.VectorAssembler(inputCols=['start_dt', \n'start_tm', \n'end_dt', \n'end_tm', \n'event_tt', \n'event_ds', \n'event_ad',  \n'end_nt', \n'Contact_ad', \n'Contact_em', \n'Contact_ph', \n'Location_ad', \n'Category',                                         \n'Registration_ad', \n'last_dr'\n], outputCol='features')"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["import pyspark.ml.classification as cl\nlogistic = cl.LogisticRegression(maxIter=10, regParam=0.01, labelCol='Maximum Seats')\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[start_dt, \n                            start_tm, \n                            end_dt, \n                            end_tm, \n                            event_tt, \n                            event_ds, \n                            event_ad,  \n                            end_nt, \n                            Contact_ad, \n                            Contact_em, \n                            Contact_ph, \n                            Location_ad, \n                            Registration_ad, \n                            last_dr,                        \n                            featurec,\n                            logistic])"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["hr_data_train, hr_data_test = df.randomSplit([0.7,0.3],seed=100)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%sh nc -vz db_hostname db_port"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%fs ls"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%fs "],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%fs ls '/FileStore/tables/'"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["rdd = spark.read.text('/FileStore/tables/sample_movielens_ratings.txt').rdd"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["print rdd.collect()\nsplitdata = rdd.map(lambda row:row.value.split(\"::\"))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["print splitdata.collect()\nfrom pyspark.sql import Row\nrdd1 = splitdata.map(lambda d:Row(id= int(d[0]), movieid= int(d[1]), rating= int(d[2]), time= long(d[3]) ))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["print rdd1.collect()\ndf = spark.createDataFrame(rdd1)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["df.show(5)\n"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["import pyspark.sql.types as typ\ndf.withColumn('id', df['id'].cast(typ.IntegerType()))\n#df.withColumn('movieid', df['movieid'].cast(typ.IntegerType()))\n#df.withColumn('rating', df['rating'].cast(typ.IntegerType()))\n"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["rdd2 = spark.read.text('/FileStore/tables/Beckman_Sample.txt').rdd"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["rdd2.take(5)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["from pyspark.sql import Row\nrdd3 = rdd2.map(lambda row: row.value.split('\\t'))\nrdd4 = rdd3.map(lambda x: Row(mmid = str(x[0]), did = str(x[2]), typeid = str(x[2]), amtvalue = float(x[3]), areapk = int(x[4]), hightpk = int(x[5])))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["rdd4.collect()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["pkdf = spark.createDataFrame(rdd4)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["pkdf.show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["pkdf.printSchema()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["pkdf.columns"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["pkdf.corr('areapk', 'hightpk', method='pearson')"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["pkdf.describe().show()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["pkdf.distinct().show()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["pkdf.dtypes"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["pkdf.explain()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["pkdf.filter('areapk==4788').show()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["pkdf.where('hightpk==2236').show()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["pkdf.schema"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["#print pkdf.storageLevel\npkdf.cache().storageLevel"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["pkdf.toPandas()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["pkdf.filter(pkdf.mmid.endswith('_MM2')).show()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["from pyspark.sql import functions as F\npkdf.select(pkdf.areapk, F.when(pkdf.did=='No', 1).otherwise(0)).show()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["rdd = sc.parallelize([\"a\", \"a\", \"b\", \"c\"])\nrdd.map(lambda x:(x, 1)).collect()"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import ChiSqSelector\ndf = spark.createDataFrame(\n  [(Vectors.dense([1.0, 2.0, 18.0, 2.0]), 1.0),\n   (Vectors.dense([19.0, 22.0, 48.0, 12.0]), 2.0),\n   (Vectors.dense([11.0, 21.0, 28.0, 23.0]), 3.0),\n   (Vectors.dense([31.0, 32.0, 38.0, 42.0]), 4.0)    \n  ],[\"features\", \"label\"]\n)\nselector = ChiSqSelector(numTopFeatures=1, outputCol=\"selectedFeatures\")\nmodel = selector.fit(df)\nmodel.transform(df).head().selectedFeatures"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["out.columns"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["out[['selectedFeatures']].show()"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":54}],"metadata":{"name":"RDDMy","notebookId":2277265930329753},"nbformat":4,"nbformat_minor":0}
