{"cells":[{"cell_type":"code","source":["rdd = sc.parallelize([1,2,3],2)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"code","source":["sc.textFile('/FileStore/tables/Baby_Names/Baby_Names.txt').collect()\nsc.textFile('/FileStore/tables/Baby_Names/Baby_Names.txt').count()\n\n\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["BabyName=sc.textFile('/FileStore/tables/Baby_Names/Baby_Names.txt')\n#BabyName.first()\nBabyName.take(10)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\ncombOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n\nprint sc.parallelize([('hello',1), ('good',2), ('hello',3), ('food',4),('food',5),('good',2)],3).aggregateByKey((0, 0), seqOp, combOp).collect()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#Using cache() persists \nrdd = sc.parallelize(range(10000)) #first time this line will be executed\nrdd.cache()\nrdd1 = rdd.map(lambda x:x+2)\nrdd2 = rdd.map(lambda x:x+3)\nprint rdd1.count()\nprint rdd2.count()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["x = sc.parallelize([(\"a\", 1), (\"b\", 4),(\"a\",9)])\ny = sc.parallelize([(\"a\",5)])\nnew_rdd = x.cogroup(y)\nnew_rdd.collect()\n[(a, map(f ,b)) for a, b in new_rdd.collect()]\n\ndef f(e):\n  return sum(list(e))\n\n\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# In MyStr\n# In MyStr\n# In MyConcat\n# In MyConcat\n# In MyStr\n# In MyStr\n# In MyConcat\n# In MyConcat\n# In myPartConcat\n\n#Invoked per partition first time a key appears, d is the corresponding value \ndef mystr(d):\n    print 'In MyStr'\n    return d\n\n# 2nd time & onwards for same key in same partition\ndef myconcat(a,b):\n    print 'In MyConcat'\n    return a + b\n\n#Works across partitions\ndef mypartConcat(a,b):\n    print 'In myPartConcat'\n    return a + b\n\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2),(\"a\",8),(\"c\",4), (\"a\", 12),(\"a\",18),(\"c\",14)],2)\n\n#mystr - this converts the V into of type C\nrdd.combineByKey(mystr, myconcat, mypartConcat).collect()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["def mystr(d):\n    print 'In MyStr'\n    return d\n\n# 2nd time & onwards for same key in same partition\ndef myconcat(a,b):\n    print 'In MyConcat'\n    return a + b\n\n#Works across partitions\ndef mypartConcat(a,b):\n    print 'In myPartConcat'\n    return (a + b)\n\n\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2),(\"a\",8),(\"c\",4), (\"a\", 12),(\"a\",18),(\"c\",14)],2)\n\n#mystr - this converts the V into of type C\nrdd1=rdd.combineByKey(mystr, myconcat, mypartConcat)\n\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["sc.textFile('/FileStore/tables/1800/1800.csv').collect()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["rdd=sc.textFile(\"/FileStore/tables/1800/1800.csv\") \\\n    .map(lambda line: line.split(\",\")) \\\n    .filter(lambda line: len(line)>1) \\\n    .map(lambda line: (line[0],line[1],line[2],line[3])) \nrdd.collect()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["sc.textFile(\"/FileStore/tables/1800/1800.csv\") \\\n    .map(lambda line: line.split(\",\")) \\\n    .filter(lambda line: len(line)>1) \\\n    .map(lambda line: (line[0],line[1],line[2],line[3])) \\\n    .collect()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"name":"Day1","notebookId":2686603785351371},"nbformat":4,"nbformat_minor":0}
